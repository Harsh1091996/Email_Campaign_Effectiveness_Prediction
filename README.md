# Email Campaign Effectiveness Prediction
These project is part of the “Machine Learning &Advanced Machine Learning” curriculum as capstone projects at [AlmaBetter](https://www.almabetter.com/). 

#### -- Project Status: [Completed]

## Objective<br>
The main objective is to create a machine learning model to characterize the mail and track the mail that is ignored; read; acknowledged by the reader. 
Most of the small to medium business owners are making effective use of Gmail-based Email marketing Strategies for offline targeting of converting their
prospective customers into leads so that they stay with them in Business.

<h2> :floppy_disk: Project Files Description</h2>

<p>This Project includes 1 executable files, 1 presentation and 1 project documentation  as follows:</p>
<h4>Executable Files:</h4>
<ul>
  <li><b>Team_3_Email_Campaign_Effectiveness_Prediction.ipynb</b> - Complete notebook containing Data exploration/Data processing/transformation/model development.</li>
</ul>

<h4>Presentation:</h4>
<ul>
  <li><b>Email Campaign Effectiveness Prediction (1).pdf</b> - Contains presentation i.e pptx of the project .</li>
</ul>

<h4>Documentation:</h4>
<ul>
  <li><b>Email Campaign Effectiveness Prediction_cohort_nilgiri.docx.pdf</b> - Includes the documentation of the project.</li>
</ul>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)


### Methods Used
* Descriptive Statistics
* Data Visualization
* Machine Learning


### Technologies
* Python
* Pandas
* Numpy
* Matplotlib
* Seaborn
* Scikit-learn
* XGBoost


## Project Description
* EDA - Performed exploratory data analysis on numerical and categorical data.
* Data Cleaning - Missing value imputation,Outlier Treaatment
* Imabalance handling - First tried Under sampling and implemented baseline models then due to loss of information moved to different technique i.e oversampling using SMOTE and got better results.
* Feature Selection - Used information gain for feature selection and dropped features which had less information gain
* Model development - Tried different model and finally compared all models F1 and roc_auc score.

## XGBoost
The library is laser focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features.

### Model Features
The implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:

**Gradient Boosting** algorithm also called gradient boosting machine including the learning rate.
**Stochastic Gradient** Boosting with sub-sampling at the row, column and column per split levels.
**Regularized Gradient Boosting** with both L1 and L2 regularization.
### System Features
The library provides a system for use in a range of computing environments, not least:

* **Parallelization** of tree construction using all of your CPU cores during training.
* **Distributed Computing** for training very large models using a cluster of machines.
* **Out-of-Core Computing** for very large datasets that don’t fit into memory.
* **Cache Optimization** of data structures and algorithm to make best use of hardware.

### Algorithm Features
The implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:

* **Sparse Aware** implementation with automatic handling of missing data values.
* **Block Structure** to support the parallelization of tree construction.
* **Continued Training** so that you can further boost an already fitted model on new data.

XGBoost is free open source software available for use under the permissive Apache-2 license.


## Needs of this project

- data exploration/descriptive statistics
- data processing/cleaning
- predictive modeling

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<h2> :clipboard: Execution Instruction</h2>
<p>The order of execution of the program files is as follows:</p>
<p><b>1) Team_3_Email_Campaign_Effectiveness_Prediction.ipynb</b></p>
<p> The Team_3_Email_Campaign_Effectiveness_Prediction.ipynb contains the entire code for Data exploration/Data processing/transformation/model development </p>


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

## Getting Started

1. Clone this repo (for help see this [tutorial](https://help.github.com/articles/cloning-a-repository/)).
2. Project documentation is being kept [here](https://github.com/Harsh1091996/Email_Campaign_Effectiveness_Prediction/blob/main/Email%20Campaign%20Effectiveness%20Prediction_cohort_nilgiri.docx.pdf) within this repo.
3. Project presentation is being kept [here](https://github.com/Harsh1091996/Email_Campaign_Effectiveness_Prediction/blob/main/Email%20Campaign%20Effectiveness%20Prediction%20(1).pdf) within this repo.    
4. Complete notebook containing Data exploration/Data processing/transformation/model development is being kept [here](https://github.com/Harsh1091996/Email_Campaign_Effectiveness_Prediction/blob/main/Team_3_Email_Campaign_Effectiveness_Prediction.ipynb)
 




<!-- CREDITS -->
<h2 id="credits"> :scroll: Credits</h2>

Himanshu Sharma | Avid Learner | Data Scientist | Machine Learning Engineer | Deep Learning enthusiast

<p> <i> Contact me for Data Science Project Collaborations</i></p>


[![LinkedIn Badge](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/himan-10/)
[![GitHub Badge](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Harsh1091996)
[![Medium Badge](https://img.shields.io/badge/Medium-1DA1F2?style=for-the-badge&logo=medium&logoColor=white)](https://harshsharma1091996.medium.com/)
[![Resume Badge](https://img.shields.io/badge/resume-0077B5?style=for-the-badge&logo=resume&logoColor=white)](https://drive.google.com/file/d/1pyTvHo2Ec4xfCszL7YkHYAwWgFi5Uf2T/view?usp=sharing)

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)
<h2> :books: References</h2>
<ul>
  <li><p>Jason Brownlee, 'Blog on XGBoost'. [Online].</p>
      <p>Available: https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/</p>
  </li>
  <li><p>Wikipedia.org, 'XGBoost'. [Online].</p>
      <p>Available: https://en.wikipedia.org/wiki/XGBoost</p>
  </li>
  <li><p>Youtube.com, 'XGBoost working'. [Online].</p>
      <p>Available: https://www.youtube.com/watch?v=OQKQHNCVf5k</p>
  </li>
  <li><p>Manisha-sirsat.blogspot.com, 'What is Confusion Matrix and Advanced Classification Metrics?'. [Online].</p>
      <p>Available: https://manisha-sirsat.blogspot.com/2019/04/confusion-matrix.html</p>
  </li>
</ul>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)
