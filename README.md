# Email Campaign Effectiveness Prediction
These project is part of the “Machine Learning &Advanced Machine Learning” curriculum as capstone projects at [AlmaBetter](https://www.almabetter.com/). 

#### -- Project Status: [Completed]

## Objective<br>
The main objective is to create a machine learning model to characterize the mail and track the mail that is ignored; read; acknowledged by the reader. 
Most of the small to medium business owners are making effective use of Gmail-based Email marketing Strategies for offline targeting of converting their
prospective customers into leads so that they stay with them in Business.



### Methods Used
* Descriptive Statistics
* Data Visualization
* Machine Learning


### Technologies
* Python
* Pandas
* Numpy
* Matplotlib
* Seaborn
* Scikit-learn
* XGBoost


## Project Description
* EDA - Performed exploratory data analysis on numerical and categorical data.
* Data Cleaning - Missing value imputation,Outlier Treaatment
* Imabalance handling - First tried Under sampling and implemented baseline models then due to loss of information moved to different technique i.e oversampling using SMOTE and got better results.
* Feature Selection - Used information gain for feature selection and dropped features which had less information gain
* Model development - Tried different model and finally compared all models F1 and roc_auc score.

## XGBoost
The library is laser focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features.

### Model Features
The implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:

**Gradient Boosting** algorithm also called gradient boosting machine including the learning rate.
**Stochastic Gradient** Boosting with sub-sampling at the row, column and column per split levels.
**Regularized Gradient Boosting** with both L1 and L2 regularization.
### System Features
The library provides a system for use in a range of computing environments, not least:

* **Parallelization** of tree construction using all of your CPU cores during training.
* **Distributed Computing** for training very large models using a cluster of machines.
* **Out-of-Core Computing** for very large datasets that don’t fit into memory.
* **Cache Optimization** of data structures and algorithm to make best use of hardware.

### Algorithm Features
The implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:

* **Sparse Aware** implementation with automatic handling of missing data values.
* **Block Structure** to support the parallelization of tree construction.
* **Continued Training** so that you can further boost an already fitted model on new data.

XGBoost is free open source software available for use under the permissive Apache-2 license.


## Needs of this project

- data exploration/descriptive statistics
- data processing/cleaning
- predictive modeling

## Getting Started

1. Clone this repo (for help see this [tutorial](https://help.github.com/articles/cloning-a-repository/)).
2. Project documentation is being kept [here](https://github.com/Harsh1091996/Email_Campaign_Effectiveness_Prediction/blob/main/Email%20Campaign%20Effectiveness%20Prediction_cohort_nilgiri.docx.pdf) within this repo.
3. Project presentation is being kept [here](https://github.com/Harsh1091996/Email_Campaign_Effectiveness_Prediction/blob/main/Email%20Campaign%20Effectiveness%20Prediction%20(1).pdf) within this repo.    
4. Complete notebook containing Data exploration/Data processing/transformation/model development is being kept [here](https://github.com/Harsh1091996/Email_Campaign_Effectiveness_Prediction/blob/main/Team_3_Email_Campaign_Effectiveness_Prediction.ipynb)
 




<!-- CREDITS -->
<h2 id="credits"> :scroll: Credits</h2>

Himanshu Sharma | Avid Learner | Data Scientist | Machine Learning Engineer | Deep Learning enthusiast

<p> <i> Contact me for Data Science Project Collaborations</i></p>


[![LinkedIn Badge](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/himan-10/)
[![GitHub Badge](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Harsh1091996)
[![Medium Badge](https://img.shields.io/badge/Medium-1DA1F2?style=for-the-badge&logo=medium&logoColor=white)](https://harshsharma1091996.medium.com/)
[![Resume Badge](https://img.shields.io/badge/resume-0077B5?style=for-the-badge&logo=resume&logoColor=white)](https://drive.google.com/file/d/1pyTvHo2Ec4xfCszL7YkHYAwWgFi5Uf2T/view?usp=sharing)
